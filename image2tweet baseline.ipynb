{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image cap on im2 tweet - baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AsilnibgsPz_",
        "2zuNmwVzgksB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsilnibgsPz_"
      },
      "source": [
        "#bert 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg1Y5K501RqF"
      },
      "source": [
        "\n",
        "!pip install transformers==3.0.2\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "#from processData import Vocabulary\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "from IPython import display\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21BrhHbVdH1_"
      },
      "source": [
        "\n",
        "\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import Counter\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFGI_xaq0KJX"
      },
      "source": [
        "\n",
        "import pickle \n",
        "with open(\"/content/drive/MyDrive/IMAGE2TWEET/train_img_vgg19.pkl\",\"rb\") as f:\n",
        "  i_e=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOZpvI4GCzhN"
      },
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/english_train_data.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dBdF4s9G2OsG",
        "outputId": "23bf6ef1-8435-4cb1-e52c-832f692dab4f"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Image URL</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cluster ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1263440694305124352</td>\n",
              "      <td>http://pbs.twimg.com/media/EYikiz5UYAAD6Mu.jpg</td>\n",
              "      <td>#CycloneAmphan: 72 people killed in West Benga...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1237032071123496962</td>\n",
              "      <td>http://pbs.twimg.com/media/ESrSBkcUwAAnfZ-.jpg</td>\n",
              "      <td>Congress party postpones their 'Gandhi Sandesh...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1237025347306225670</td>\n",
              "      <td>http://pbs.twimg.com/media/ESqhtA_U8AEwrMQ.jpg</td>\n",
              "      <td>Congress general secretary Mukul Wasnik marrie...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1237025194767601665</td>\n",
              "      <td>http://pbs.twimg.com/media/ESrL2MYUcAA2eNA.jpg</td>\n",
              "      <td>Techie tested positive for coronavirus in Beng...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1237024936415211520</td>\n",
              "      <td>http://pbs.twimg.com/media/ESrLnrUUMAITOV6.jpg</td>\n",
              "      <td>Man who returned from Italy confirmed positive...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48787</th>\n",
              "      <td>1148971878783963137</td>\n",
              "      <td>http://pbs.twimg.com/media/D_H307DU0AUts5P.jpg</td>\n",
              "      <td>#WC2019WithTimes #INDvsNZ #INDvNZ #CWC19 #INDv...</td>\n",
              "      <td>28469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48788</th>\n",
              "      <td>1148904697853464577</td>\n",
              "      <td>http://pbs.twimg.com/media/D_GowoEUYAAtVXa.jpg</td>\n",
              "      <td>Deposits in Jan Dhan accounts cross Rs 1 lakh ...</td>\n",
              "      <td>28470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48789</th>\n",
              "      <td>1148889598396944385</td>\n",
              "      <td>http://pbs.twimg.com/media/D_GciluUwAU7XmU.png</td>\n",
              "      <td>.@RahulGandhi's follower count on Twitter cros...</td>\n",
              "      <td>28471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48790</th>\n",
              "      <td>1148788937110016001</td>\n",
              "      <td>http://pbs.twimg.com/media/D_FMNhgU0AAIt-1.jpg</td>\n",
              "      <td>Concerns over government's facial recognition ...</td>\n",
              "      <td>28472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48791</th>\n",
              "      <td>1148782347384262656</td>\n",
              "      <td>http://pbs.twimg.com/media/D_FLch2U8AAljq2.jpg</td>\n",
              "      <td>You share your birthday with... https://t.co/u...</td>\n",
              "      <td>28473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>48792 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Tweet ID  ... Cluster ID\n",
              "0      1263440694305124352  ...          1\n",
              "1      1237032071123496962  ...          2\n",
              "2      1237025347306225670  ...          3\n",
              "3      1237025194767601665  ...          4\n",
              "4      1237024936415211520  ...          5\n",
              "...                    ...  ...        ...\n",
              "48787  1148971878783963137  ...      28469\n",
              "48788  1148904697853464577  ...      28470\n",
              "48789  1148889598396944385  ...      28471\n",
              "48790  1148788937110016001  ...      28472\n",
              "48791  1148782347384262656  ...      28473\n",
              "\n",
              "[48792 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH-OfivY0KF7"
      },
      "source": [
        "a=list(i_e.keys())\n",
        "x=df1.columns\n",
        "q=list(df1['Text'])\n",
        "caps={}\n",
        "for i in a:\n",
        "  j=int(i.split(\"_\")[1])\n",
        "  caps[i]=['startseq ' + q[j]+ ' endseq']\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHCChLO70KCu"
      },
      "source": [
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key]\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                se= [word for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    encoded = tokenizer.batch_encode_plus(\n",
        "                                                    [\" \".join(se[:i])],\n",
        "                                                add_special_tokens=True,\n",
        "                                                  max_length=61,\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_token_type_ids=True,\n",
        "                                                 pad_to_max_length=True,\n",
        "                                                   return_tensors=\"tf\",\n",
        "                                                     truncation=True\n",
        "                                                )\n",
        "                    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "                    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "                    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "                    sequence_output, pooled_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "                    b1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n",
        "                    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(b1)\n",
        "                    max_pool = tf.keras.layers.GlobalMaxPooling1D()(b1)\n",
        "                    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "                    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "                    # split into input and output pair\n",
        "                    out_seq = seq[i]\n",
        "                    # pad input sequence\n",
        "                    #in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(dropout)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            if n==num_photos_per_batch:\n",
        "                yield ([np.array(X1), np.array(X2)], np.array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyk8fdCp0J-x",
        "outputId": "383309f9-f27d-4cac-bb3b-926932ec8e4a"
      },
      "source": [
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in list(caps.values()):\n",
        "    nsents += 1\n",
        "    for w in sent[0].split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary = 9587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibszc-Dl0J8d"
      },
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMFZN320J59"
      },
      "source": [
        "caption_max_length = 33\n",
        "batch_size = 1\n",
        "max_length = 33\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_1pVOz0ToU"
      },
      "source": [
        "generator = data_generator(caps, i_e, wordtoix, max_length, batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGpsaJ5Q0Tk6"
      },
      "source": [
        "epochs = 3\n",
        "batch_size = 3\n",
        "steps = len(caps)//batch_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhOGnZmF0TiZ",
        "outputId": "7d2e05ca-0468-4825-aae5-45020554e1bf"
      },
      "source": [
        "\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "\n",
        "inputs2 = Input(shape=(256,))\n",
        "\n",
        "decoder1 = add([fe2,inputs2])#add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 4096)         0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 256)          1048832     ['dropout_124[0][0]']            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 256)          0           ['dense_3[0][0]',                \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 256)          65792       ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 9588)         2464116     ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,578,740\n",
            "Trainable params: 3,578,740\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpWl-j9s0TgI"
      },
      "source": [
        "model.fit_generator(generator, epochs=20, steps_per_epoch=steps, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaL4AmdlGSqk",
        "outputId": "da57162f-bd87-48ad-cf56-ec64b1e6de4d"
      },
      "source": [
        "%cd /content/drive/MyDrive/IMAGE2TWEET/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/11Sw-O0lrFYAF-DUvpGN2x_IdU7YnPfZG/IMAGE2TWEET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qks3jjJb0J3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda8c2c8-91a8-4d3e-a0a9-30986892709c"
      },
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "save_model(\n",
        "    model,\n",
        "    'models/bert_vgg19_IM2TW.hdf5',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TKZi-aa5DLM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_1XYbwB5DHp"
      },
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        #sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        #sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        #add bert here\n",
        "        encoded = tokenizer.batch_encode_plus(\n",
        "                                                    [in_text],\n",
        "                                                add_special_tokens=True,\n",
        "                                                  max_length=61,\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_token_type_ids=True,\n",
        "                                                 pad_to_max_length=True,\n",
        "                                                   return_tensors=\"tf\",\n",
        "                                                     truncation=True\n",
        "                                                )\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "        sequence_output, pooled_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "        b1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(b1)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(b1)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "\n",
        "\n",
        "        yhat = model.predict([photo,dropout], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xnSrC7p7rtZ"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/IMAGE2TWEET/val_img_vgg19.pkl\",\"rb\") as f:\n",
        "    i_e=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKhnzs1O5DA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbxZACo5C-i"
      },
      "source": [
        "\n",
        "ret=[]\n",
        "z=list(i_e.values())\n",
        "for i in range(len(z)):\n",
        "  print(i)\n",
        "  image=z[i]\n",
        "  ret.append(greedySearch(image))\n",
        "#ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CXVyyKbcYfQ"
      },
      "source": [
        "df=pd.DataFrame(ret,columns=['caps'])\n",
        "df.to_excel(\"/content/drive/MyDrive/IMAGE2TWEET/exp_bert_caps.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXrLWzxUcYXp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocoevalcap\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "\n",
        "!pip install nltk==3.5\n",
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "gIDNU9wuSW8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDzMWyjgTn3v"
      },
      "source": [
        "class I2TTokeninser:\n",
        "  def tokenise(self,captions):\n",
        "      id=[k for k,v in captions.items()]\n",
        "      PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\",\".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"]\n",
        "      caps=[v for k,v in captions.items()] ### [ [caption 1_1,caption 1_2 ,] ,[caption 2_1,caption 2_2,..],..]\n",
        "      tok_cap=[]\n",
        "      for i in range(len(caps)):\n",
        "        temp=[]\n",
        "        for j in range(len(caps[i])):\n",
        "          tokenised_caps=' '.join([w for w in caps[i][j].rstrip().split(' ') if w not in PUNCTUATIONS])\n",
        "          temp.append(tokenised_caps)\n",
        "        tok_cap.append(temp)\n",
        "      ret_caps={}\n",
        "      for i in range(len(id)):\n",
        "        ret_caps[id[i]]=tok_cap[i]\n",
        "      return ret_caps\n",
        " \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaoMm-15YkhS"
      },
      "source": [
        "\n",
        "\n",
        "def scorer(orig,ret):\n",
        "  tok=I2TTokeninser()\n",
        "  ori=tok.tokenise(orig)\n",
        "  re=tok.tokenise(ret)\n",
        "\n",
        "\n",
        "  Avg_cider=0.0\n",
        "  Avg_bleu=0\n",
        "  Avg_meteor=0\n",
        "  Avg_rouge=0\n",
        "  \n",
        "  for i in range(len(re.keys())):\n",
        "    temp_re={}\n",
        "    temp_re[list(re.keys())[i]]=list(re.values())[i]#[list(re.values())[i]]\n",
        "    caps=ori[list(re.keys())[i]]\n",
        "\n",
        "    for j in range(len(caps)):\n",
        "      temp={}\n",
        "      temp[list(re.keys())[i]]=[caps[j]]\n",
        "\n",
        "      score,_=Cider().compute_score(temp, temp_re)\n",
        "      Avg_cider=Avg_cider + score \n",
        "    \n",
        "    Avg_cider= Avg_cider +  Avg_cider/len(caps)   ## getting average score of every ret caption adn referance caption(captions in cluster) and adding to total score(for all retrieved captions)\n",
        "  Avg_cider=Avg_cider / len(re.keys())\n",
        "  score, scores = Cider().compute_score(ori, re)\n",
        "  print(\"cider \",score)\n",
        "  Mscore=0\n",
        "  for i in range(len(re.keys())):\n",
        "    name=list(re.keys())[i]\n",
        "    Mscore =Mscore +nltk.translate.meteor_score.meteor_score(ori[name], re[name][0])\n",
        "  Mscore =Mscore /len(re.keys())\n",
        "  \n",
        "  Avg_meteor=Mscore  \n",
        "\n",
        "\n",
        "  score,_=Rouge().compute_score(ori, re)\n",
        "  Avg_rouge=score\n",
        "\n",
        "  BLEUscore = 0\n",
        "  for i in range(len(re.keys())):\n",
        "    name=list(re.keys())[i]\n",
        "    #x=[]\n",
        "    #for j in range(len(ori[name])):\n",
        "    #  x.append(ori[name][j].split())\n",
        "    #BLEUscore =BLEUscore +nltk.translate.bleu_score.sentence_bleu(x, re[name][0].split())\n",
        "    BLEUscore =BLEUscore +nltk.translate.bleu_score.sentence_bleu(ori[name], re[name][0])\n",
        "  BLEUscore =BLEUscore /len(re.keys())\n",
        "  Avg_bleu=BLEUscore\n",
        "  #print(\"CIDER: \",\"{0:.4f}\".format(Avg_cider))\n",
        "  print(\"Meteor : \",Avg_rouge,\"ROUGE : \" ,Avg_rouge,\"BLEU 4 :  \" ,Avg_bleu) # return score \n",
        "import pandas as pd\n",
        "\n",
        "def evaluator(ret):\n",
        "  #ret === > {image_id : [caption], } example : ret={1300483489071063042:['black animal'],1300465872838950912:['dinner at a restraunt']}\n",
        "\n",
        "  \n",
        "  \n",
        "  df1=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/english_test_data (1).csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  id=[k for k,v in ret.items()]\n",
        "  #caps=[v for k,v in ret.items()] \n",
        "  orig={}\n",
        "  for i in range(len(id)):\n",
        "    cluster_id=list(df1[df1['Tweet ID']==id[i]]['Cluster ID'])[0]\n",
        "    c=df1[df1['Cluster ID']==cluster_id]['Text']\n",
        "    temp=[]\n",
        "    for j in c:\n",
        "      temp.append(j)\n",
        "    orig[id[i]]=temp\n",
        "  #print(orig)\n",
        "  #print(ret)\n",
        "  scorer(orig,ret)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=ret\n",
        "ret={}\n",
        "df2=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/english_test_data (1).csv\")\n",
        "\n",
        "x=df2['id']\n",
        "\n",
        "for i in range(0,len(x)):\n",
        "  ret[x[i]]=[y[i]]"
      ],
      "metadata": {
        "id": "yfptvuCyvW9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLGM7JP0JVBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f17ee8-bbb0-47dc-fc4c-29b9bf1d99c7"
      },
      "source": [
        "evaluator(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cider  3.756380705535758e-04\n",
            "Meteor :  0.00013100512749380385 ROUGE :  0.00013100512749380385 BLEU 4 :   0.023742007300135464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2hqHp0g0J1U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QDubGxNLgkJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zuNmwVzgksB"
      },
      "source": [
        "#bert 3 Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onsatTOwgksC"
      },
      "source": [
        "\n",
        "!pip install transformers==3.0.2\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "#from processData import Vocabulary\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "from IPython import display\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1lZ8_49gksD"
      },
      "source": [
        "\n",
        "\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import Counter\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfGxY1ESgksE"
      },
      "source": [
        "\n",
        "import pickle \n",
        "with open(\"/content/drive/MyDrive/IMAGE2TWEET/hindi_train_img_vgg19.pkl\",\"rb\") as f:\n",
        "  i_e=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsvD3Uy5gksE"
      },
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/hindi_train_data.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "b9730fdb-5d82-4869-8b83-029a4320ee3a",
        "id": "TVOU090MgksF"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8e23e921-e757-444b-a9db-759bb2beab99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Image URL</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cluster ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>765816844875800576</td>\n",
              "      <td>http://pbs.twimg.com/media/CqC6IKaUIAEk81F.jpg</td>\n",
              "      <td>विदेश नीति में बड़ा बदलाव? #US को खुश करने के ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>765801793355911168</td>\n",
              "      <td>http://pbs.twimg.com/media/CqCsiARXgAAUFu-.jpg</td>\n",
              "      <td>#Ladakh में 14000 फीट ऊपर मिली 10000 साल पुरान...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>765801486043508736</td>\n",
              "      <td>http://pbs.twimg.com/media/CqCr2m-WgAQQVvz.jpg</td>\n",
              "      <td>#Bihar में जहरीली शराब पीने से 13 की मौत, 4 मह...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>765787288068616192</td>\n",
              "      <td>http://pbs.twimg.com/media/CqCfPl0XEAEcr8l.jpg</td>\n",
              "      <td>#maharashtra के दबंग MLA ने कलेक्टर को मारे थप...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>765779586496000000</td>\n",
              "      <td>http://pbs.twimg.com/media/CqCYHz7WEAARY8v.jpg</td>\n",
              "      <td>#China के खिलाफ #भारत ने बढ़ाई ताकत: लद्दाख मे...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35696</th>\n",
              "      <td>607047306739777536</td>\n",
              "      <td>http://pbs.twimg.com/media/CGyqTrDUYAM7TkD.jpg</td>\n",
              "      <td>रोचक है टेलीविजन का इतिहास, इन बातों को शायद आ...</td>\n",
              "      <td>32865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35697</th>\n",
              "      <td>607047239203094528</td>\n",
              "      <td>http://pbs.twimg.com/media/CGyqPxNUAAAc2ed.jpg</td>\n",
              "      <td>रोचक है टेलीविजन का इतिहास, इन बातों को शायद आ...</td>\n",
              "      <td>32866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35698</th>\n",
              "      <td>607047059103825920</td>\n",
              "      <td>http://pbs.twimg.com/media/CGyqFSZVIAAE5wF.jpg</td>\n",
              "      <td>रोचक है टेलीविजन का इतिहास, इन बातों को शायद आ...</td>\n",
              "      <td>32867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35699</th>\n",
              "      <td>606724304596631552</td>\n",
              "      <td>http://pbs.twimg.com/media/CGuEcoTUQAA-KuF.jpg</td>\n",
              "      <td>MONSOON: मानसून ने केरल में दी दस्तक, अगले दो ...</td>\n",
              "      <td>32868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35700</th>\n",
              "      <td>605994821803634688</td>\n",
              "      <td>http://pbs.twimg.com/media/CGjtEx8UkAAe0Cb.jpg</td>\n",
              "      <td>#Maruti ने लॉन्च की 30 के माइलेज वाली #Celerio...</td>\n",
              "      <td>32869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35701 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e23e921-e757-444b-a9db-759bb2beab99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e23e921-e757-444b-a9db-759bb2beab99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e23e921-e757-444b-a9db-759bb2beab99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Tweet ID  ... Cluster ID\n",
              "0      765816844875800576  ...          1\n",
              "1      765801793355911168  ...          2\n",
              "2      765801486043508736  ...          3\n",
              "3      765787288068616192  ...          4\n",
              "4      765779586496000000  ...          5\n",
              "...                   ...  ...        ...\n",
              "35696  607047306739777536  ...      32865\n",
              "35697  607047239203094528  ...      32866\n",
              "35698  607047059103825920  ...      32867\n",
              "35699  606724304596631552  ...      32868\n",
              "35700  605994821803634688  ...      32869\n",
              "\n",
              "[35701 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpaTXwffgksF"
      },
      "source": [
        "a=list(i_e.keys())\n",
        "x=df1.columns\n",
        "q=list(df1['Text'])\n",
        "caps={}\n",
        "for i in a:\n",
        "  j=int(i.split(\"_\")[1])\n",
        "  caps[i]=['startseq ' + q[j]+ ' endseq']\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isGWgnxugksG"
      },
      "source": [
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key]\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                se= [word for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    encoded = tokenizer.batch_encode_plus(\n",
        "                                                    [\" \".join(se[:i])],\n",
        "                                                add_special_tokens=True,\n",
        "                                                  max_length=61,\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_token_type_ids=True,\n",
        "                                                 pad_to_max_length=True,\n",
        "                                                   return_tensors=\"tf\",\n",
        "                                                     truncation=True\n",
        "                                                )\n",
        "                    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "                    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "                    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "                    sequence_output, pooled_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "                    b1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n",
        "                    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(b1)\n",
        "                    max_pool = tf.keras.layers.GlobalMaxPooling1D()(b1)\n",
        "                    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "                    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "                    # split into input and output pair\n",
        "                    out_seq = seq[i]\n",
        "                    # pad input sequence\n",
        "                    #in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(dropout)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            if n==num_photos_per_batch:\n",
        "                #yield ([np.array(X1), np.array(X2)], np.array(y))\n",
        "                yield ([np.array(X1).reshape(-1,4096), np.array(X2).reshape(-1,256)], np.array(y).reshape(-1,7334))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394b62cc-8dbf-4759-89d8-fecebadbea1c",
        "id": "TBu9VZiHgksH"
      },
      "source": [
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in list(caps.values()):\n",
        "    nsents += 1\n",
        "    for w in sent[0].split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary = 7333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cq24rUzgksH"
      },
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Zx1ZntgksI"
      },
      "source": [
        "caption_max_length = 33\n",
        "batch_size = 1\n",
        "max_length = 33\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJrf9J96gksI"
      },
      "source": [
        "generator = data_generator(caps, i_e, wordtoix, max_length, batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QGFTJEYgksK"
      },
      "source": [
        "epochs = 3\n",
        "batch_size = 3\n",
        "steps = len(caps)//batch_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab436ce-78b1-4fe6-bd93-bc9f50fce7de",
        "id": "osdKp7ragksT"
      },
      "source": [
        "\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "\n",
        "inputs2 = Input(shape=(256,))\n",
        "\n",
        "\n",
        "decoder1 = add([fe2,inputs2])#add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 4096)         0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          1048832     ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 7334)         1884838     ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,999,462\n",
            "Trainable params: 2,999,462\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj9HeHZlgksT"
      },
      "source": [
        "model.fit_generator(generator, epochs=20, steps_per_epoch=steps, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7204f4de-8129-4cf8-f47f-48e73b64dc8b",
        "id": "kK6ccn4DgksW"
      },
      "source": [
        "%cd /content/drive/MyDrive/IMAGE2TWEET/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/11Sw-O0lrFYAF-DUvpGN2x_IdU7YnPfZG/IMAGE2TWEET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707b7d14-32fd-4b1b-db77-0e230906812b",
        "id": "GwssMSDGgksX"
      },
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "save_model(\n",
        "    model,\n",
        "    'models/HINDI_bert_vgg19_IM2TW.hdf5',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC10VEesgksX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLoC_Yr9gksX"
      },
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        #sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        #sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        #add bert here\n",
        "        encoded = tokenizer.batch_encode_plus(\n",
        "                                                    [in_text],\n",
        "                                                add_special_tokens=True,\n",
        "                                                  max_length=61,\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_token_type_ids=True,\n",
        "                                                 pad_to_max_length=True,\n",
        "                                                   return_tensors=\"tf\",\n",
        "                                                     truncation=True\n",
        "                                                )\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "        sequence_output, pooled_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "        b1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(b1)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(b1)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "\n",
        "\n",
        "        yhat = model.predict([photo,dropout], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bv8ffFqgksY"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/IMAGE2TWEET/hindi_test_img_vgg19.pkl\",\"rb\") as f:\n",
        "    i_e=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA913UifgksZ"
      },
      "source": [
        "ret=[]\n",
        "z=list(i_e.values())\n",
        "for i in range(len(z)):\n",
        "  print(i)\n",
        "  image=z[i]\n",
        "  ret.append(greedySearch(image))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x27RAPg7tdfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocoevalcap\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "\n",
        "!pip install nltk==3.5\n",
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "ZZ7ys3Ayt2vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmklVExt2vc"
      },
      "source": [
        "class I2TTokeninser:\n",
        "  def tokenise(self,captions):\n",
        "      id=[k for k,v in captions.items()]\n",
        "      PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\",\".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"]\n",
        "      caps=[v for k,v in captions.items()] ### [ [caption 1_1,caption 1_2 ,] ,[caption 2_1,caption 2_2,..],..]\n",
        "      tok_cap=[]\n",
        "      for i in range(len(caps)):\n",
        "        temp=[]\n",
        "        for j in range(len(caps[i])):\n",
        "          tokenised_caps=' '.join([w for w in caps[i][j].rstrip().split(' ') if w not in PUNCTUATIONS])\n",
        "          temp.append(tokenised_caps)\n",
        "        tok_cap.append(temp)\n",
        "      ret_caps={}\n",
        "      for i in range(len(id)):\n",
        "        ret_caps[id[i]]=tok_cap[i]\n",
        "      return ret_caps\n",
        " \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oceHmhdmt2vd"
      },
      "source": [
        "\n",
        "\n",
        "def scorer(orig,ret):\n",
        "  tok=I2TTokeninser()\n",
        "  ori=tok.tokenise(orig)\n",
        "  re=tok.tokenise(ret)\n",
        "\n",
        "\n",
        "  Avg_cider=0.0\n",
        "  Avg_bleu=0\n",
        "  Avg_meteor=0\n",
        "  Avg_rouge=0\n",
        "  \n",
        "  for i in range(len(re.keys())):\n",
        "    temp_re={}\n",
        "    temp_re[list(re.keys())[i]]=list(re.values())[i]#[list(re.values())[i]]\n",
        "    caps=ori[list(re.keys())[i]]\n",
        "\n",
        "    for j in range(len(caps)):\n",
        "      temp={}\n",
        "      temp[list(re.keys())[i]]=[caps[j]]\n",
        "\n",
        "      score,_=Cider().compute_score(temp, temp_re)\n",
        "      Avg_cider=Avg_cider + score \n",
        "    \n",
        "    Avg_cider= Avg_cider +  Avg_cider/len(caps)   ## getting average score of every ret caption adn referance caption(captions in cluster) and adding to total score(for all retrieved captions)\n",
        "  Avg_cider=Avg_cider / len(re.keys())\n",
        "  score, scores = Cider().compute_score(ori, re)\n",
        "  print(\"cider \",score)\n",
        "  Mscore=0\n",
        "  for i in range(len(re.keys())):\n",
        "    name=list(re.keys())[i]\n",
        "    Mscore =Mscore +nltk.translate.meteor_score.meteor_score(ori[name], re[name][0])\n",
        "  Mscore =Mscore /len(re.keys())\n",
        "  \n",
        "  Avg_meteor=Mscore  \n",
        "\n",
        "\n",
        "  score,_=Rouge().compute_score(ori, re)\n",
        "  Avg_rouge=score\n",
        "\n",
        "  BLEUscore = 0\n",
        "  for i in range(len(re.keys())):\n",
        "    name=list(re.keys())[i]\n",
        "    #x=[]\n",
        "    #for j in range(len(ori[name])):\n",
        "    #  x.append(ori[name][j].split())\n",
        "    #BLEUscore =BLEUscore +nltk.translate.bleu_score.sentence_bleu(x, re[name][0].split())\n",
        "    BLEUscore =BLEUscore +nltk.translate.bleu_score.sentence_bleu(ori[name], re[name][0])\n",
        "  BLEUscore =BLEUscore /len(re.keys())\n",
        "  Avg_bleu=BLEUscore\n",
        "  #print(\"CIDER: \",\"{0:.4f}\".format(Avg_cider))\n",
        "  print(\"Meteor : \",Avg_rouge,\"ROUGE : \" ,Avg_rouge,\"BLEU 4 :  \" ,Avg_bleu) # return score \n",
        "import pandas as pd\n",
        "\n",
        "def evaluator(ret):\n",
        "  #ret === > {image_id : [caption], } example : ret={1300483489071063042:['black animal'],1300465872838950912:['dinner at a restraunt']}\n",
        "\n",
        "  df1=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/hindi_test_data (1).csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  id=[k for k,v in ret.items()]\n",
        "  #caps=[v for k,v in ret.items()] \n",
        "  orig={}\n",
        "  for i in range(len(id)):\n",
        "    cluster_id=list(df1[df1['Tweet ID']==id[i]]['Cluster ID'])[0]\n",
        "    c=df1[df1['Cluster ID']==cluster_id]['Text']\n",
        "    temp=[]\n",
        "    for j in c:\n",
        "      temp.append(j)\n",
        "    orig[id[i]]=temp\n",
        "  #print(orig)\n",
        "  #print(ret)\n",
        "  scorer(orig,ret)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=ret\n",
        "r={}\n"
      ],
      "metadata": {
        "id": "jHaGlf2UOCV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=pd.read_csv(\"/content/drive/MyDrive/IMAGE2TWEET/hindi_test_data (1).csv\")\n",
        "e=df2.columns\n",
        "\n",
        "x=df2[e[0]]\n"
      ],
      "metadata": {
        "id": "yy61NjdYOEM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "f86VeQZsD7U-",
        "outputId": "edfff64f-1a61-47c8-a2e4-94847e928c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8c91b2c4-7798-4d39-bf3a-2d2665eadc85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>Image URL</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cluster ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>672060201063714816</td>\n",
              "      <td>http://pbs.twimg.com/media/CVOjMMFUsAAzuu1.jpg</td>\n",
              "      <td>Today's Headlines@ 8 PM\\r\\nरिपोर्ट में दावा: I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>672035790541598720</td>\n",
              "      <td>http://pbs.twimg.com/media/CVOM_VQUwAABF3k.jpg</td>\n",
              "      <td>‘बाजीराव-मस्तानी’ के नए पोस्टर में दिखा दीपिका...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>672029674831110144</td>\n",
              "      <td>http://pbs.twimg.com/media/CVOHbWRVEAEpvKv.jpg</td>\n",
              "      <td>क्रिमिनल्स के छक्के छुड़ाने वाले इस CSP ने रिट...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>672017633508638721</td>\n",
              "      <td>http://pbs.twimg.com/media/CVN8eaYVEAAH9_d.jpg</td>\n",
              "      <td>दुनिया के सबसे यंगेस्ट खरबपति की जिंदगी में बे...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>672009054810562560</td>\n",
              "      <td>http://pbs.twimg.com/media/CVN0q_GUsAATffq.jpg</td>\n",
              "      <td>एक विस्फोट...और देखते ही देखते हवा में उड़े ची...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7647</th>\n",
              "      <td>877044934351568896</td>\n",
              "      <td>http://pbs.twimg.com/media/DCvjhwMUQAA4nK_.jpg</td>\n",
              "      <td>1993 बॉम्बे ब्लास्ट: CBI ने सलेम को छोड़कर 5 द...</td>\n",
              "      <td>7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7648</th>\n",
              "      <td>914537416190578688</td>\n",
              "      <td>http://pbs.twimg.com/media/DLEWycXUQAAWJTT.jpg</td>\n",
              "      <td>'गोरखपुर को जल्द मिलेगा इंटरनेशनल स्टेडियम, 5 ...</td>\n",
              "      <td>7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7649</th>\n",
              "      <td>1098199948221243393</td>\n",
              "      <td>http://pbs.twimg.com/media/Dz2XBCwUYAUal4f.jpg</td>\n",
              "      <td>कंफर्म / क्वालकॉम ने बनाई दूसरी पीढ़ी की 5 जी ...</td>\n",
              "      <td>7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7650</th>\n",
              "      <td>1122845186210459648</td>\n",
              "      <td>http://pbs.twimg.com/media/D5UlsapU0AEFIAp.jpg</td>\n",
              "      <td>लोकसभा चुनाव / 5 बजे तक 54% वोटिंग: अफसर को धम...</td>\n",
              "      <td>7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7651</th>\n",
              "      <td>1139065318024683523</td>\n",
              "      <td>http://pbs.twimg.com/media/D87FyQrUIAAvCM5.png</td>\n",
              "      <td>साहो /1.39 मिनट के टीजर में दिखे गन्स और गाड़ि...</td>\n",
              "      <td>7043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7652 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c91b2c4-7798-4d39-bf3a-2d2665eadc85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8c91b2c4-7798-4d39-bf3a-2d2665eadc85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8c91b2c4-7798-4d39-bf3a-2d2665eadc85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Tweet ID  ... Cluster ID\n",
              "0      672060201063714816  ...          1\n",
              "1      672035790541598720  ...          2\n",
              "2      672029674831110144  ...          3\n",
              "3      672017633508638721  ...          4\n",
              "4      672009054810562560  ...          5\n",
              "...                   ...  ...        ...\n",
              "7647   877044934351568896  ...       7043\n",
              "7648   914537416190578688  ...       7043\n",
              "7649  1098199948221243393  ...       7043\n",
              "7650  1122845186210459648  ...       7043\n",
              "7651  1139065318024683523  ...       7043\n",
              "\n",
              "[7652 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0,len(y)):\n",
        "  r[x[i]]=[y[i]]  "
      ],
      "metadata": {
        "id": "rZHl3Y_EvF64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41a8d1b-dfdd-46a0-a516-58b188a4bae6",
        "id": "RkcBA4hct2ve"
      },
      "source": [
        "import nltk\n",
        "\n",
        "evaluator(r)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cider  4.958281440697747e-04 \n",
            " Meteor :  0.0023126010193659234 ROUGE :  0.00023126010193659234 BLEU 4 :   0.033913476601767118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lHWz3qKxtdXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XLj1oqH6tdRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QG4573wgksc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}